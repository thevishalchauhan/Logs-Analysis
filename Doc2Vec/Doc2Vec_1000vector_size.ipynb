{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all the above stundents \n",
    "from tqdm import tqdm\n",
    "# tqdm is for printing the status bar\n",
    "def textpreprocess(sentance):\n",
    "    #remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", str(sentance)).strip()\n",
    "    #remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n",
    "    stoplist = set(stopwords.words('english'))\n",
    "    #remove stop words\n",
    "    sentance = [e.lower() for e in sentance.split() if e.lower() not in stoplist]\n",
    "    return sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v, [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log_Data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['mlu', 'e', 'status', 'device', 'ready', 'n',...</td>\n",
       "      <td>DeviceNotReady_BelowMR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['deferred', 'odntodo', 'odnwip', 'n', 'mlu', ...</td>\n",
       "      <td>DeviceNotReady_BelowMR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['user', 'kdriver', 'starting', 'input', 'powe...</td>\n",
       "      <td>DeviceNotReady_BelowMR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['mlu', 'mlu', 'adding', 'waiter', 'obj', 'run...</td>\n",
       "      <td>DeviceNotReady_BelowMR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['user', 'tldlistener', 'start', 'operation', ...</td>\n",
       "      <td>MCC_BadCRC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Log_Data                   Label\n",
       "0  ['mlu', 'e', 'status', 'device', 'ready', 'n',...  DeviceNotReady_BelowMR\n",
       "1  ['deferred', 'odntodo', 'odnwip', 'n', 'mlu', ...  DeviceNotReady_BelowMR\n",
       "2  ['user', 'kdriver', 'starting', 'input', 'powe...  DeviceNotReady_BelowMR\n",
       "3  ['mlu', 'mlu', 'adding', 'waiter', 'obj', 'run...  DeviceNotReady_BelowMR\n",
       "4  ['user', 'tldlistener', 'start', 'operation', ...              MCC_BadCRC"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_logs_preprocessed.csv', usecols = [1,2])\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.Log_Data, df.Label, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"Doc2VecModel\\d2v_1000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec with vector size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model, len(X_train), 1000, 'Train')\n",
    "test_vectors_dbow = get_vectors(model, len(X_test), 1000, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.51286131,  0.24583107, -0.13014413, ..., -0.10543926,\n",
       "        -0.01474785,  0.05175322],\n",
       "       [ 0.03392103, -0.38855737, -0.06742401, ...,  0.07175257,\n",
       "         0.01580577,  0.06185701]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vectors_dbow[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression ( 91.6% Accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9166666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                            MCR       0.00      0.00      0.00         1\n",
      "                raid_data_error       0.67      1.00      0.80         2\n",
      "Weka_Collects_Miniport_Slowness       1.00      1.00      1.00         2\n",
      "                     MCC_BadCRC       1.00      1.00      1.00         2\n",
      "                  MCC_CacheLost       1.00      1.00      1.00         3\n",
      "         DeviceNotReady_BelowMR       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.92      0.92      0.92        12\n",
      "                      macro avg       0.78      0.83      0.80        12\n",
      "                   weighted avg       0.86      0.92      0.88        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=list(set(df.Label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Y_test            Y_pred\n",
      "------------------------\n",
      "MCR  ==  MCR\n",
      "MCR  ==  MCR\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "raid_data_error  ==  raid_data_error\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "!!!! DeviceNotReady_BelowMR  !=  MCC_BadCRC\n",
      "raid_data_error  ==  raid_data_error\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "print(\"Y_test            Y_pred\")\n",
    "print(\"------------------------\")\n",
    "for i,j in zip(y_test,y_pred):\n",
    "    if i == j:\n",
    "        print(i+\"  ==  \"+j)\n",
    "    else:\n",
    "        print(\"!!!! \"+i+\"  !=  \"+j)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (83.3% Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8333333333333334\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                            MCR       0.00      0.00      0.00         1\n",
      "                raid_data_error       0.50      0.50      0.50         2\n",
      "Weka_Collects_Miniport_Slowness       1.00      1.00      1.00         2\n",
      "                     MCC_BadCRC       1.00      1.00      1.00         2\n",
      "                  MCC_CacheLost       1.00      1.00      1.00         3\n",
      "         DeviceNotReady_BelowMR       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.83      0.83      0.83        12\n",
      "                      macro avg       0.75      0.75      0.75        12\n",
      "                   weighted avg       0.83      0.83      0.83        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(max_iter=100, tol=1e-3, random_state = 101)\n",
    "sgd.fit(train_vectors_dbow, y_train)\n",
    "y_pred_sgd = sgd.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred_sgd, y_test))\n",
    "print(classification_report(y_test, y_pred_sgd,target_names=list(set(df.Label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Y_test            Y_pred\n",
      "------------------------\n",
      "MCR  ==  MCR\n",
      "MCR  ==  MCR\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "raid_data_error  ==  raid_data_error\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "!!! MCC_BadCRC  !=  DeviceNotReady_BelowMR\n",
      "!!! DeviceNotReady_BelowMR  !=  MCC_BadCRC\n",
      "raid_data_error  ==  raid_data_error\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "print(\"Y_test            Y_pred\")\n",
    "print(\"------------------------\")\n",
    "for i,j in zip(y_test,y_pred_sgd):\n",
    "    if i == j:\n",
    "        print(i+\"  ==  \"+j)\n",
    "    else:\n",
    "        print(\"!!! \"+i+\"  !=  \"+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours (66.6% Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                            MCR       0.00      0.00      0.00         1\n",
      "                raid_data_error       0.00      0.00      0.00         2\n",
      "Weka_Collects_Miniport_Slowness       1.00      0.50      0.67         2\n",
      "                     MCC_BadCRC       0.67      1.00      0.80         2\n",
      "                  MCC_CacheLost       0.60      1.00      0.75         3\n",
      "         DeviceNotReady_BelowMR       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.67      0.67      0.67        12\n",
      "                      macro avg       0.54      0.58      0.54        12\n",
      "                   weighted avg       0.59      0.67      0.60        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=len(set(df.Label)))\n",
    "neigh.fit(train_vectors_dbow, y_train)\n",
    "y_pred_neigh = neigh.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred_neigh, y_test))\n",
    "print(classification_report(y_test, y_pred_neigh,target_names=list(set(df.Label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Y_test            Y_pred\n",
      "------------------------\n",
      "MCR  ==  MCR\n",
      "MCR  ==  MCR\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "!!!! MCC_BadCRC  !=  Weka_Collects_Miniport_Slowness\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "raid_data_error  ==  raid_data_error\n",
      "!!!! MCC_CacheLost  !=  MCR\n",
      "!!!! MCC_BadCRC  !=  DeviceNotReady_BelowMR\n",
      "!!!! DeviceNotReady_BelowMR  !=  Weka_Collects_Miniport_Slowness\n",
      "raid_data_error  ==  raid_data_error\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "print(\"Y_test            Y_pred\")\n",
    "print(\"------------------------\")\n",
    "for i,j in zip(y_test,y_pred_neigh):\n",
    "    if i == j:\n",
    "        print(i+\"  ==  \"+j)\n",
    "    else:\n",
    "        print(\"!!!! \"+i+\"  !=  \"+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier (33.3% Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                            MCR       0.00      0.00      0.00         1\n",
      "                raid_data_error       0.50      0.50      0.50         2\n",
      "Weka_Collects_Miniport_Slowness       0.20      0.50      0.29         2\n",
      "                     MCC_BadCRC       0.00      0.00      0.00         2\n",
      "                  MCC_CacheLost       0.00      0.00      0.00         3\n",
      "         DeviceNotReady_BelowMR       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.33      0.33      0.33        12\n",
      "                      macro avg       0.28      0.33      0.30        12\n",
      "                   weighted avg       0.28      0.33      0.30        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(random_state=101, max_features = None, max_depth = 10, min_samples_leaf = 4)\n",
    "dtree.fit(train_vectors_dbow, y_train)\n",
    "y_pred_dtree = dtree.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred_dtree, y_test))\n",
    "print(classification_report(y_test, y_pred_dtree,target_names=list(set(df.Label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Y_test            Y_pred\n",
      "------------------------\n",
      "!!!! MCR  !=  MCC_CacheLost\n",
      "!!!! MCR  !=  MCC_CacheLost\n",
      "!!!! MCC_CacheLost  !=  DeviceNotReady_BelowMR\n",
      "!!!! MCC_BadCRC  !=  MCC_CacheLost\n",
      "!!!! Weka_Collects_Miniport_Slowness  !=  DeviceNotReady_BelowMR\n",
      "raid_data_error  ==  raid_data_error\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "!!!! DeviceNotReady_BelowMR  !=  MCC_BadCRC\n",
      "raid_data_error  ==  raid_data_error\n",
      "!!!! Weka_Collects_Miniport_Slowness  !=  MCC_CacheLost\n",
      "!!!! Weka_Collects_Miniport_Slowness  !=  DeviceNotReady_BelowMR\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "print(\"Y_test            Y_pred\")\n",
    "print(\"------------------------\")\n",
    "for i,j in zip(y_test,y_pred_dtree):\n",
    "    if i == j:\n",
    "        print(i+\"  ==  \"+j)\n",
    "    else:\n",
    "        print(\"!!!! \"+i+\"  !=  \"+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest (33.3% Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                            MCR       0.00      0.00      0.00         1\n",
      "                raid_data_error       0.50      0.50      0.50         2\n",
      "Weka_Collects_Miniport_Slowness       0.20      0.50      0.29         2\n",
      "                     MCC_BadCRC       0.00      0.00      0.00         2\n",
      "                  MCC_CacheLost       0.00      0.00      0.00         3\n",
      "         DeviceNotReady_BelowMR       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.33      0.33      0.33        12\n",
      "                      macro avg       0.28      0.33      0.30        12\n",
      "                   weighted avg       0.28      0.33      0.30        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfm = RandomForestClassifier(n_estimators=100, max_depth=10,random_state=101, oob_score = True, max_features = None,\n",
    "                             min_samples_leaf = 4, n_jobs = -1)\n",
    "rfm.fit(train_vectors_dbow, y_train)\n",
    "y_pred_rfm = dtree.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred_rfm, y_test))\n",
    "print(classification_report(y_test, y_pred_rfm,target_names=list(set(df.Label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Y_test            Y_pred\n",
      "------------------------\n",
      "!!!! MCR  !=  MCC_CacheLost\n",
      "!!!! MCR  !=  MCC_CacheLost\n",
      "!!!! MCC_CacheLost  !=  DeviceNotReady_BelowMR\n",
      "!!!! MCC_BadCRC  !=  MCC_CacheLost\n",
      "!!!! Weka_Collects_Miniport_Slowness  !=  DeviceNotReady_BelowMR\n",
      "raid_data_error  ==  raid_data_error\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "!!!! DeviceNotReady_BelowMR  !=  MCC_BadCRC\n",
      "raid_data_error  ==  raid_data_error\n",
      "!!!! Weka_Collects_Miniport_Slowness  !=  MCC_CacheLost\n",
      "!!!! Weka_Collects_Miniport_Slowness  !=  DeviceNotReady_BelowMR\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "print(\"Y_test            Y_pred\")\n",
    "print(\"------------------------\")\n",
    "for i,j in zip(y_test,y_pred_rfm):\n",
    "    if i == j:\n",
    "        print(i+\"  ==  \"+j)\n",
    "    else:\n",
    "        print(\"!!!! \"+i+\"  !=  \"+j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (83.3 %  Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8333333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "                            MCR       0.00      0.00      0.00         1\n",
      "                raid_data_error       0.67      1.00      0.80         2\n",
      "Weka_Collects_Miniport_Slowness       1.00      0.50      0.67         2\n",
      "                     MCC_BadCRC       0.67      1.00      0.80         2\n",
      "                  MCC_CacheLost       1.00      1.00      1.00         3\n",
      "         DeviceNotReady_BelowMR       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.83      0.83      0.83        12\n",
      "                      macro avg       0.72      0.75      0.71        12\n",
      "                   weighted avg       0.81      0.83      0.79        12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear', random_state = 101, C = 1)\n",
    "svm.fit(train_vectors_dbow, y_train)\n",
    "y_pred_svm = svm.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred_svm, y_test))\n",
    "print(classification_report(y_test, y_pred_svm,target_names=list(set(df.Label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Y_test            Y_pred\n",
      "------------------------\n",
      "MCR  ==  MCR\n",
      "MCR  ==  MCR\n",
      "MCC_CacheLost  ==  MCC_CacheLost\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "raid_data_error  ==  raid_data_error\n",
      "!!!! MCC_CacheLost  !=  MCR\n",
      "MCC_BadCRC  ==  MCC_BadCRC\n",
      "!!!! DeviceNotReady_BelowMR  !=  MCC_BadCRC\n",
      "raid_data_error  ==  raid_data_error\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n",
      "Weka_Collects_Miniport_Slowness  ==  Weka_Collects_Miniport_Slowness\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------\")\n",
    "print(\"Y_test            Y_pred\")\n",
    "print(\"------------------------\")\n",
    "for i,j in zip(y_test,y_pred_svm):\n",
    "    if i == j:\n",
    "        print(i+\"  ==  \"+j)\n",
    "    else:\n",
    "        print(\"!!!! \"+i+\"  !=  \"+j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
